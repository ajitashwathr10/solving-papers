{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc93aa62-0e30-444a-9808-4771855d7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521a841b-f2f7-48d4-b202-0b415ff60ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(Q, K, V, mask = None):\n",
    "    matmul_qk = tf.matmul(Q, K, transpose_b = True) #Uses to compute dot product (.) of two matrices\n",
    "    d_k = tf.cast(tf.shape(K)[-1], tf.float32) #Extracts the size of the last dimension of tensor and to float32\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n",
    "    #This to prevent overly large values that could destablize training\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9) #-1e9 = -1000000000\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)\n",
    "    #Converts the logits into a probabilisitc distribution where each element is either a 0 or 1\n",
    "    #Sum of these probabilistics = 1\n",
    "    output = tf.matmul(attention_weights, V)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ec5935-5bfb-46f0-81ae-09ab3a87565a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nScaled Dot-Product Attention\\nIt is a way to determine how much focus or attention that the each part of an input sequences\\nshould have on every other sides while producing a prediction.\\n\\nParameters: Q: Queries, K: Keys, V: Values\\n    - The query represent the part of the inputs to be given attention.\\n    - The key helps deciding how part of the input is related to query\\n    - The value holds actual information you want to use after getting the 'focus'\\n\\nWorking Methodology:\\n    - Dot Products: Compute a dot product between query and key. This result is the score for how much attention\\n    do each key gets in relative with its query\\n    - Scale: There are many times when the dot products can go upto large, large values. So, you need to\\n    divide them by the square root of 'dk' to keep the model stable and working fine.\\n    - Softmax: This is an activation function, actually. So, what it does is, it turns these scores into\\n    probabilities (or call them attention weights). These weights tell you how much each value contribute\\n    into the final output.\\n    - Weighted Sum: For the desired output, we have to multiply the attention weights by the values and\\n    sum up them together.\\n\\nFormula:\\n    Attention(Q; K; V) = softmax(QKT / sqrt(dk)) * V\\n    Here:\\n        QKT: Dot product of queries and keys.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Scaled Dot-Product Attention\n",
    "It is a way to determine how much focus or attention that the each part of an input sequences\n",
    "should have on every other sides while producing a prediction.\n",
    "\n",
    "Parameters: Q: Queries, K: Keys, V: Values\n",
    "    - The query represent the part of the inputs to be given attention.\n",
    "    - The key helps deciding how part of the input is related to query\n",
    "    - The value holds actual information you want to use after getting the 'focus'\n",
    "\n",
    "Working Methodology:\n",
    "    - Dot Products: Compute a dot product between query and key. This result is the score for how much attention\n",
    "    do each key gets in relative with its query\n",
    "    - Scale: There are many times when the dot products can go upto large, large values. So, you need to\n",
    "    divide them by the square root of 'dk' to keep the model stable and working fine.\n",
    "    - Softmax: This is an activation function, actually. So, what it does is, it turns these scores into\n",
    "    probabilities (or call them attention weights). These weights tell you how much each value contribute\n",
    "    into the final output.\n",
    "    - Weighted Sum: For the desired output, we have to multiply the attention weights by the values and\n",
    "    sum up them together.\n",
    "\n",
    "Formula:\n",
    "    Attention(Q; K; V) = softmax(QKT / sqrt(dk)) * V\n",
    "    Here:\n",
    "        QKT: Dot product of queries and keys.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "618cebf0-c248-482e-8041-c70d444554e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-Head Attention\n",
    "class MultiHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        #Linear layers for projections\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        self.W_o = tf.keras.layers.Dense(d_model)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_k))\n",
    "        return tf.transpose(x, perm = [0, 2, 1, 3])\n",
    "\n",
    "    def call(self, Q, K, V, mask = None):\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        #Linear projections\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        #These projections split into heads\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.W_o(concat_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca9fd0c3-35af-4478-be83-357c2a0ee413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMulti-Head Attention\\nIt is the way to split the attention process into multiple smaller \"heads\" (Linear projections) to capture\\ndifferent aspects of the inputs.\\n\\nDifference between Scaled Attention v Multi-Head Attention\\n    - Scaled Attention works with one set of Q: Queries, K: Keys, and V: Values to calculate the output. \\n    These are all typically the same size, say \\'d_model\\'.\\n    - In Multi-Head Attention, we break this model into smaller parts. (If we had 8 heads, each head\\n    will work with 64)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Multi-Head Attention\n",
    "It is the way to split the attention process into multiple smaller \"heads\" (Linear projections) to capture\n",
    "different aspects of the inputs.\n",
    "\n",
    "Difference between Scaled Attention v Multi-Head Attention\n",
    "    - Scaled Attention works with one set of Q: Queries, K: Keys, and V: Values to calculate the output. \n",
    "    These are all typically the same size, say 'd_model'.\n",
    "    - In Multi-Head Attention, we break this model into smaller parts. (If we had 8 heads, each head\n",
    "    will work with 64)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15f3e511-d15b-46ce-a253-638fa4c075c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Position-Wise Feed-Forward Networks\n",
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(d_ff, activation = 'relu') #W_1, b_1\n",
    "        self.dense2 = tf.keras.layers.Dense(d_model) #W_2, b_2\n",
    "    def call(self, x):\n",
    "        return self.dense2(self.dense1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8fef32e-73ae-4fed-a82e-bdf8e07bf423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPosition-Wise Feed-Forward Networks\\n\\nEach layer of both the encoder and decoder has a feed-forward network (also known as FFN).\\n\\nWorking Methodology:\\n    - Each position (which can considered as word or token) in the input is processed independently using\\n    the same function.\\n    - This function is consisting of two linear transformations which is done by multiply by weights with\\n    a 'ReLU' activation.\\n\\nFormula:\\n    - FFN(x) = max(0, xW1 + b1)W2 + b2\\n    Here,\\n        - W1, W2: Weight matrices\\n        - b1, b2: Bias \\n        - ReLU: Non-linear activations\\nPoints:\\n    - Every position in this input is all processed individually but in the same way, irrespective of \\n    its position.\\n    - The weights here changes from layer to layer, but remains same in all positions.\\n    - Input and output dimensions are 512, while intermediate layers has 2048 dimensions.\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Position-Wise Feed-Forward Networks\n",
    "\n",
    "Each layer of both the encoder and decoder has a feed-forward network (also known as FFN).\n",
    "\n",
    "Working Methodology:\n",
    "    - Each position (which can considered as word or token) in the input is processed independently using\n",
    "    the same function.\n",
    "    - This function is consisting of two linear transformations which is done by multiply by weights with\n",
    "    a 'ReLU' activation.\n",
    "\n",
    "Formula:\n",
    "    - FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "    Here,\n",
    "        - W1, W2: Weight matrices\n",
    "        - b1, b2: Bias \n",
    "        - ReLU: Non-linear activations\n",
    "Points:\n",
    "    - Every position in this input is all processed individually but in the same way, irrespective of \n",
    "    its position.\n",
    "    - The weights here changes from layer to layer, but remains same in all positions.\n",
    "    - Input and output dimensions are 512, while intermediate layers has 2048 dimensions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90cc7b1f-92db-4282-9ad2-4aba64bad617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddings and Softmax\n",
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        embed *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        #Scaling here by sqrt method of d_model\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d921963-185a-4136-92fa-40512d42ee89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nToken Embedding and Weight Sharing\\n\\n- This model is used to convert words or tokens into numerical vectors using embeddings.\\n- Each of these tokens is turned into a vector of size 'd_model' (which is roughly 512 dimensions) using\\nlearned embeddings.\\n- In the end of decoder, this model uses a linear transformation + softmax function to predict the\\nnext word in the given sequences which comes in this model.\\n- Weight Matrix\\n    - Input embeddings\\n    - Output embeddings\\n    - Final transformation which is used to predict the next token\\n- Scaling Trick: Embeddings is multipled by (root 512) to used the maintain the numerical stability.\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Token Embedding and Weight Sharing\n",
    "\n",
    "- This model is used to convert words or tokens into numerical vectors using embeddings.\n",
    "- Each of these tokens is turned into a vector of size 'd_model' (which is roughly 512 dimensions) using\n",
    "learned embeddings.\n",
    "- In the end of decoder, this model uses a linear transformation + softmax function to predict the\n",
    "next word in the given sequences which comes in this model.\n",
    "- Weight Matrix\n",
    "    - Input embeddings\n",
    "    - Output embeddings\n",
    "    - Final transformation which is used to predict the next token\n",
    "- Scaling Trick: Embeddings is multipled by (root 512) to used the maintain the numerical stability.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69b8633f-5c76-4e91-b2ce-1b71e2e83d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positional Encoding\n",
    "def positional_encoding(max_seq_len, d_model):\n",
    "    angles = np.arange(max_seq_len)[:, np.newaxis] / np.power(\n",
    "        10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "    pos_encoding = np.zeros(angles.shape)\n",
    "    pos_encoding[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "\n",
    "    return tf.cast(pos_encoding[np.newaxis, ...], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "486beef4-8b55-4efb-83ab-c17c2ae0f6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPositional Encoding \\n\\nThis model currently does not understand the words or tokens in a sentence, because it doesn't use recurrence\\n(which indefinitely means the Recurrent Neural Networks) or the convolutions (like Convolutional Neural\\nNetworks). To fix the bug, we add positional encoding to the embeddings so that the model can understand\\nthe position of each word in the sequences.\\n\\nWorking Methodology:\\n1. Adding Positional Stuff\\n    - Each token is converted into a vector\\n    - These encodings are added to these embeddings to give model the understanding of order.\\n    - Positional Encoding == Positional Embeddings (roughly 512 dimensions)\\n2. Using Sin & Cos waves. Formula will be given down below.\\n\\nFormula:\\n    PE(pos, 2i) = sin(pos / 10000^2i/d_model) #Sine Wave\\n    PE(pos, 2i + 1) = cos(pos / 10000^2i/d_model) #Cosine Wave\\n\\nThese are used to create unique and weird patterns for each positions.\\nThey also allow the model to learn the relative based positions like how far apart are these words are, easily.\\nThis pattern continues which allows to handle longer sequences than the process of doing in training.\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Positional Encoding \n",
    "\n",
    "This model currently does not understand the words or tokens in a sentence, because it doesn't use recurrence\n",
    "(which indefinitely means the Recurrent Neural Networks) or the convolutions (like Convolutional Neural\n",
    "Networks). To fix the bug, we add positional encoding to the embeddings so that the model can understand\n",
    "the position of each word in the sequences.\n",
    "\n",
    "Working Methodology:\n",
    "1. Adding Positional Stuff\n",
    "    - Each token is converted into a vector\n",
    "    - These encodings are added to these embeddings to give model the understanding of order.\n",
    "    - Positional Encoding == Positional Embeddings (roughly 512 dimensions)\n",
    "2. Using Sin & Cos waves. Formula will be given down below.\n",
    "\n",
    "Formula:\n",
    "    PE(pos, 2i) = sin(pos / 10000^2i/d_model) #Sine Wave\n",
    "    PE(pos, 2i + 1) = cos(pos / 10000^2i/d_model) #Cosine Wave\n",
    "\n",
    "These are used to create unique and weird patterns for each positions.\n",
    "They also allow the model to learn the relative based positions like how far apart are these words are, easily.\n",
    "This pattern continues which allows to handle longer sequences than the process of doing in training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "350ca562-90a2-489c-a952-c59c13baba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Stacks\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask = None):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training = training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training = training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e24173c-487a-496d-bcdf-ab8960b8fb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEncoder - Transformer\\n\\nThe encoder consists of 6 identical layers, each with two key components:\\n    - Multi-Head Self-Attention: This helps model focus on the relevant words in the input.\\n    - Feed-Forward Network: A fully connected layer applied to each position separately.\\n\\nInstead of just passing the output of each layer to the next, we add original input back.\\nThis helps avoid vanishing gradients and makes training easier.\\n\\nFormula:\\n    - Output = LayerNorm(x + Sublayer(x))\\n    Here,\\n        - x: Input to sub-layer\\n        - Sublayer(x): Self-attention or feed-forward network\\n        - LayerNorm: Normalizes the values to keep the model stable.\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Encoder - Transformer\n",
    "\n",
    "The encoder consists of 6 identical layers, each with two key components:\n",
    "    - Multi-Head Self-Attention: This helps model focus on the relevant words in the input.\n",
    "    - Feed-Forward Network: A fully connected layer applied to each position separately.\n",
    "\n",
    "Instead of just passing the output of each layer to the next, we add original input back.\n",
    "This helps avoid vanishing gradients and makes training easier.\n",
    "\n",
    "Formula:\n",
    "    - Output = LayerNorm(x + Sublayer(x))\n",
    "    Here,\n",
    "        - x: Input to sub-layer\n",
    "        - Sublayer(x): Self-attention or feed-forward network\n",
    "        - LayerNorm: Normalizes the values to keep the model stable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28e0da79-7d05-44fa-b55c-c299ca48adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder Stack\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate = 0.1):\n",
    "        super().__init()\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "    \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "    \n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask = None, padding_mask = None):\n",
    "        attn1, _ = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training = training)\n",
    "        out1 = self.layernorm1(x + attn1)\n",
    "\n",
    "        attn2, _ = self.mha2(out1, enc_output, enc_output, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training = training)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training = training)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a4ee5-fd78-4036-8c8c-f3618894639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
