{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc93aa62-0e30-444a-9808-4771855d7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521a841b-f2f7-48d4-b202-0b415ff60ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(Q, K, V, mask = None):\n",
    "    matmul_qk = tf.matmul(Q, K, transpose_b = True) #Uses to compute dot product (.) of two matrices\n",
    "    d_k = tf.cast(tf.shape(K)[-1], tf.float32) #Extracts the size of the last dimension of tensor and to float32\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n",
    "    #This to prevent overly large values that could destablize training\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9) #-1e9 = -1000000000\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)\n",
    "    #Converts the logits into a probabilisitc distribution where each element is either a 0 or 1\n",
    "    #Sum of these probabilistics = 1\n",
    "    output = tf.matmul(attention_weights, V)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86ec5935-5bfb-46f0-81ae-09ab3a87565a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nScaled Dot-Product Attention\\nIt is a way to determine how much focus or attention that the each part of an input sequences\\nshould have on every other sides while producing a prediction.\\n\\nParameters: Q: Queries, K: Keys, V: Values\\n    - The query represent the part of the inputs to be given attention.\\n    - The key helps deciding how part of the input is related to query\\n    - The value holds actual information you want to use after getting the 'focus'\\n\\nWorking Methodology:\\n    - Dot Products: Compute a dot product between query and key. This result is the score for how much attention\\n    do each key gets in relative with its query\\n    - Scale: There are many times when the dot products can go upto large, large values. So, you need to\\n    divide them by the square root of 'dk' to keep the model stable and working fine.\\n    - Softmax: This is an activation function, actually. So, what it does is, it turns these scores into\\n    probabilities (or call them attention weights). These weights tell you how much each value contribute\\n    into the final output.\\n    - Weighted Sum: For the desired output, we have to multiply the attention weights by the values and\\n    sum up them together.\\n\\nFormula:\\n    Attention(Q; K; V) = softmax(QKT / sqrt(dk)) * V\\n    Here:\\n        QKT: Dot product of queries and keys.\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Scaled Dot-Product Attention\n",
    "It is a way to determine how much focus or attention that the each part of an input sequences\n",
    "should have on every other sides while producing a prediction.\n",
    "\n",
    "Parameters: Q: Queries, K: Keys, V: Values\n",
    "    - The query represent the part of the inputs to be given attention.\n",
    "    - The key helps deciding how part of the input is related to query\n",
    "    - The value holds actual information you want to use after getting the 'focus'\n",
    "\n",
    "Working Methodology:\n",
    "    - Dot Products: Compute a dot product between query and key. This result is the score for how much attention\n",
    "    do each key gets in relative with its query\n",
    "    - Scale: There are many times when the dot products can go upto large, large values. So, you need to\n",
    "    divide them by the square root of 'dk' to keep the model stable and working fine.\n",
    "    - Softmax: This is an activation function, actually. So, what it does is, it turns these scores into\n",
    "    probabilities (or call them attention weights). These weights tell you how much each value contribute\n",
    "    into the final output.\n",
    "    - Weighted Sum: For the desired output, we have to multiply the attention weights by the values and\n",
    "    sum up them together.\n",
    "\n",
    "Formula:\n",
    "    Attention(Q; K; V) = softmax(QKT / sqrt(dk)) * V\n",
    "    Here:\n",
    "        QKT: Dot product of queries and keys.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "618cebf0-c248-482e-8041-c70d444554e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-Head Attention\n",
    "class MultiHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        #Linear layers for projections\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        self.W_o = tf.keras.layers.Dense(d_model)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_k))\n",
    "        return tf.transpose(x, perm = [0, 2, 1, 3])\n",
    "\n",
    "    def call(self, Q, K, V, mask = None):\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        #Linear projections\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        #These projections split into heads\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.W_o(concat_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca9fd0c3-35af-4478-be83-357c2a0ee413",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2869645389.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[27], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Multi-Head Attention\n",
    "It is the way to split the attention process into multiple smaller \"heads\" (Linear projections) to capture\n",
    "different aspects of the inputs.\n",
    "\n",
    "Difference between Scaled Attention v Multi-Head Attention\n",
    "    - Scaled Attention works with one set of Q: Queries, K: Keys, and V: Values to calculate the output. \n",
    "    These are all typically the same size, say 'd_model'.\n",
    "    - In Multi-Head Attention, we break this model into smaller parts. (If we had 8 heads, each head\n",
    "    will work with 64)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3e511-d15b-46ce-a253-638fa4c075c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
